{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ac17ee",
   "metadata": {},
   "source": [
    "# **Bayes Classifier: A Comprehensive Analysis and Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db52b1",
   "metadata": {},
   "source": [
    "\n",
    "## **Introduction**\n",
    "In this project, we explore the application of the Bayes classifier in a pattern recognition problem. \n",
    "Our objective is to classify data points into one of three distinct classes based on their features.\n",
    "The dataset consists of three classes, each represented by a set of two features, which likely correspond \n",
    "to measurements or observations in a specific domain. The Bayes classifier will be implemented to predict \n",
    "the class membership of new, unseen data points.\n",
    "\n",
    "## **Problem Domain**\n",
    "We have three classes of data points, each stored in separate text files (`Class1.txt`, `Class2.txt`, `Class3.txt`). \n",
    "Our goal is to develop a classifier that can accurately assign new data points to one of these three classes. \n",
    "To achieve this, we will implement a Bayes classifier and assess its performance.\n",
    "\n",
    "The steps involved in this project include:\n",
    "\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Data Preprocessing\n",
    "3. Checking Assumptions of the Bayes Classifier\n",
    "4. Implementing the Bayes Classifier\n",
    "5. Evaluating the Model's Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1b4f5",
   "metadata": {},
   "source": [
    "\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "Before implementing the classifier, it is essential to explore and understand the data.\n",
    "\n",
    "### **Loading and Visualizing the Data**\n",
    "We start by loading the data from the text files and visualizing it to gain insights into the distributions \n",
    "and separability of the classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c331d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Program Files\\Positron\\resources\\app\\extensions\\positron-python\\python_files\\positron\\positron_language_server.py\", line 145, in <module>\n",
      "    loop.run_forever()\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\alpine\\AppData\\Local\\Temp\\ipykernel_34072\\1467759719.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"C:\\anaconda3\\envs\\dev-ml\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n",
      "\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n",
      "\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n",
      "\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n",
      "\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n",
      "\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n",
      "\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n",
      "\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "\u001b[1;31mImportError\u001b[0m: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "class1 = pd.read_csv('/data/Class1.txt', sep=\" \", header=None)\n",
    "class2 = pd.read_csv('/data/Class2.txt', sep=\" \", header=None)\n",
    "class3 = pd.read_csv('/data/Class3.txt', sep=\" \", header=None)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(class1[0], class1[1], color='red', label='Class 1')\n",
    "plt.scatter(class2[0], class2[1], color='blue', label='Class 2')\n",
    "plt.scatter(class3[0], class3[1], color='green', label='Class 3')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Scatter Plot of Class Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017550a",
   "metadata": {},
   "source": [
    "\n",
    "### **Observations**\n",
    "1. **Data Distribution**: The scatter plot allows us to observe how each class is distributed in the feature space, \n",
    "   helping to identify any overlaps or separability between the classes.\n",
    "2. **Cluster Formation**: The plot also reveals any natural clusters within the data that correspond to the different classes.\n",
    "3. **Outliers**: Outliers can be identified visually, which might affect the performance of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5035c",
   "metadata": {},
   "source": [
    "\n",
    "## **Data Preprocessing**\n",
    "To ensure that the data is in a suitable form for the Bayes classifier, we preprocess it accordingly.\n",
    "\n",
    "### **Steps in Data Preprocessing**\n",
    "\n",
    "1. **Normalization/Standardization**: We standardize the data to have zero mean and unit variance.\n",
    "2. **Handling Missing Data**: Although not present in this dataset, missing data would be handled by imputation or removal.\n",
    "3. **Data Splitting**: We split the data into training and testing sets to evaluate the classifier's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "class1_scaled = scaler.fit_transform(class1)\n",
    "class2_scaled = scaler.fit_transform(class2)\n",
    "class3_scaled = scaler.fit_transform(class3)\n",
    "\n",
    "# Combine data from all classes\n",
    "data = pd.concat([class1, class2, class3], ignore_index=True)\n",
    "labels = [0]*len(class1) + [1]*len(class2) + [2]*len(class3)  # 0 for Class 1, 1 for Class 2, 2 for Class 3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea450e",
   "metadata": {},
   "source": [
    "\n",
    "## **Check Assumptions of the Bayes Classifier**\n",
    "Before implementing the Bayes classifier, we verify that its assumptions are met.\n",
    "\n",
    "### **Gaussian Distribution of Features**\n",
    "The Bayes classifier assumes that the features follow a Gaussian (normal) distribution within each class. \n",
    "We check this assumption by plotting histograms of the features for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba860d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(class1_scaled[:, 0], color='red', kde=True, label='Class 1 Feature 1', stat='density')\n",
    "sns.histplot(class2_scaled[:, 0], color='blue', kde=True, label='Class 2 Feature 1', stat='density')\n",
    "sns.histplot(class3_scaled[:, 0], color='green', kde=True, label='Class 3 Feature 1', stat='density')\n",
    "plt.legend()\n",
    "plt.title('Feature 1 Distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(class1_scaled[:, 1], color='red', kde=True, label='Class 1 Feature 2', stat='density')\n",
    "sns.histplot(class2_scaled[:, 1], color='blue', kde=True, label='Class 2 Feature 2', stat='density')\n",
    "sns.histplot(class3_scaled[:, 1], color='green', kde=True, label='Class 3 Feature 2', stat='density')\n",
    "plt.legend()\n",
    "plt.title('Feature 2 Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbf3b7",
   "metadata": {},
   "source": [
    "\n",
    "### **Observations**\n",
    "The histograms indicate whether the feature distributions approximate a normal distribution. \n",
    "If deviations from normality are observed, data transformations may be considered.\n",
    "\n",
    "### **Class Independence (for Naive Bayes)**\n",
    "If implementing a Naive Bayes classifier, the features are assumed to be conditionally independent given the class. \n",
    "We can examine the correlations between features within each class to verify this assumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99053126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Correlation Matrix for Class 1:\")\n",
    "print(np.corrcoef(class1_scaled[:, 0], class1_scaled[:, 1]))\n",
    "\n",
    "print(\"Correlation Matrix for Class 2:\")\n",
    "print(np.corrcoef(class2_scaled[:, 0], class2_scaled[:, 1]))\n",
    "\n",
    "print(\"Correlation Matrix for Class 3:\")\n",
    "print(np.corrcoef(class3_scaled[:, 0], class3_scaled[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4230c8",
   "metadata": {},
   "source": [
    "\n",
    "### **Class Priors**\n",
    "We estimate the prior probabilities of each class based on the training data. This step ensures that the classifier accounts for any class imbalances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_counts = np.bincount(y_train)\n",
    "class_priors = class_counts / len(y_train)\n",
    "print(\"Class Priors:\", class_priors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9d6fd",
   "metadata": {},
   "source": [
    "\n",
    "## **Conclusion**\n",
    "The exploratory data analysis, data preprocessing, and assumption checking are critical steps before implementing the Bayes classifier. \n",
    "By ensuring that the data is well-prepared and that the classifier's assumptions are reasonably met, we increase the likelihood of achieving good classification performance. \n",
    "With these preparations, we can confidently proceed to implement the Bayes classifier and evaluate its effectiveness on the test data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
